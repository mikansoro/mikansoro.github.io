<!doctype html><html lang=en><head><meta charset=utf-8><meta content="width=device-width,initial-scale=1.0" name=viewport><meta content="light dark" name=color-scheme><title>Using LLMs to Transcribe and Translate (Part 1)</title><link href=/img/favicon-32x32.png rel=icon sizes=32x32 type=image/png><link href=/img/favicon-16x16.png rel=icon sizes=16x16 type=image/png><link href=/img/apple-touch-icon.png rel=apple-touch-icon sizes=180x180><style>body{--primary-color:#ff7f27;--primary-pale-color:#ff7f2710;--text-color:#3c4043;--text-pale-color:#929ab0;--bg-color:#eee;--highlight-mark-color:#5f75b045;--callout-note-color:#5871a2;--callout-important-color:#8062b0;--callout-warning-color:#936e51;--callout-alert-color:#bc5252;--callout-question-color:#477389;--callout-tip-color:#3c8460;--main-font:ui-sans-serif,system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";--code-font:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;--homepage-max-width:750px;--main-max-width:750px;--avatar-size:70px;--paragraph-font-size:16px;--paragraph-line-height:1.75;--aside-font-size:15px;--img-border-radius:0;--inline-code-border-radius:2px}body.dark{--primary-color:#ff7f27;--primary-pale-color:#ff7f2720;--text-color:#9197a5;--text-pale-color:#626975;--bg-color:#202124;--highlight-mark-color:#5f75b045;--callout-note-color:#5871a2;--callout-important-color:#8062b0;--callout-warning-color:#936e51;--callout-alert-color:#bc5252;--callout-question-color:#477389;--callout-tip-color:#3c8460}</style><link href=/main.css rel=stylesheet><link href=/hl-light.css id=hl rel=stylesheet><body class=post><script>if(sessionStorage.getItem('theme')=='dark'){document.body.classList.add('dark');const a=document.querySelector('link#hl');if(a)a.href='/hl-dark.css'}</script><header class=blur><div id=header-wrapper><nav><a href=/>mikansoro</a><button aria-label="toggle expand" class=separator id=toggler>::</button><span class="wrap left fold">{</span><a href=/blog/>blog</a><span class="wrap-separator fold">,</span><a class=fold href=/about/>about</a><span class="wrap-separator fold">,</span><a class=fold href=/blogroll/>blogroll</a><span class="wrap right fold">} ;</span></nav><div id=btns><a aria-label="rss feed" href=/blog//feed.xml><svg viewbox="0 0 24 24" height=24 width=24 xmlns=http://www.w3.org/2000/svg><path d="M3 17C5.20914 17 7 18.7909 7 21H3V17ZM3 10C9.07513 10 14 14.9249 14 21H12C12 16.0294 7.97056 12 3 12V10ZM3 3C12.9411 3 21 11.0589 21 21H19C19 12.1634 11.8366 5 3 5V3Z" fill=currentColor></path></svg></a><button aria-label="theme switch" data-moon-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24"><path d="M10 7C10 10.866 13.134 14 17 14C18.9584 14 20.729 13.1957 21.9995 11.8995C22 11.933 22 11.9665 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12C2 6.47715 6.47715 2 12 2C12.0335 2 12.067 2 12.1005 2.00049C10.8043 3.27098 10 5.04157 10 7ZM4 12C4 16.4183 7.58172 20 12 20C15.0583 20 17.7158 18.2839 19.062 15.7621C18.3945 15.9187 17.7035 16 17 16C12.0294 16 8 11.9706 8 7C8 6.29648 8.08133 5.60547 8.2379 4.938C5.71611 6.28423 4 8.9417 4 12Z" fill="currentColor"></path></svg>' data-sun-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24"><path d="M12 18C8.68629 18 6 15.3137 6 12C6 8.68629 8.68629 6 12 6C15.3137 6 18 8.68629 18 12C18 15.3137 15.3137 18 12 18ZM12 16C14.2091 16 16 14.2091 16 12C16 9.79086 14.2091 8 12 8C9.79086 8 8 9.79086 8 12C8 14.2091 9.79086 16 12 16ZM11 1H13V4H11V1ZM11 20H13V23H11V20ZM3.51472 4.92893L4.92893 3.51472L7.05025 5.63604L5.63604 7.05025L3.51472 4.92893ZM16.9497 18.364L18.364 16.9497L20.4853 19.0711L19.0711 20.4853L16.9497 18.364ZM19.0711 3.51472L20.4853 4.92893L18.364 7.05025L16.9497 5.63604L19.0711 3.51472ZM5.63604 16.9497L7.05025 18.364L4.92893 20.4853L3.51472 19.0711L5.63604 16.9497ZM23 11V13H20V11H23ZM4 11V13H1V11H4Z" fill="currentColor"></path></svg>' id=theme-toggle><svg viewbox="0 0 24 24" height=24 width=24 xmlns=http://www.w3.org/2000/svg><path d="M10 7C10 10.866 13.134 14 17 14C18.9584 14 20.729 13.1957 21.9995 11.8995C22 11.933 22 11.9665 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12C2 6.47715 6.47715 2 12 2C12.0335 2 12.067 2 12.1005 2.00049C10.8043 3.27098 10 5.04157 10 7ZM4 12C4 16.4183 7.58172 20 12 20C15.0583 20 17.7158 18.2839 19.062 15.7621C18.3945 15.9187 17.7035 16 17 16C12.0294 16 8 11.9706 8 7C8 6.29648 8.08133 5.60547 8.2379 4.938C5.71611 6.28423 4 8.9417 4 12Z" fill=currentColor></path></svg></button><button aria-label="table of content" id=toc-toggle><svg viewbox="0 0 24 24" height=24 width=24 xmlns=http://www.w3.org/2000/svg><path d="M3 4H21V6H3V4ZM3 11H15V13H3V11ZM3 18H21V20H3V18Z" fill=currentColor></path></svg></button></div></div></header><div id=wrapper><div id=blank></div><aside class=blur><nav><ul><li><a class=h2 href=#environment>Environment</a> <ul><li><a class=h3 href=#dependency-hell>Dependency Hell</a><li><a class=h3 href=#gpu-acceleration>GPU Acceleration</a></ul><li><a class=h2 href=#transcription>Transcription</a><li><a class=h2 href=#translation>Translation</a></ul></nav><button aria-label="back to top" id=back-to-top><svg viewbox="0 0 24 24" height=24 width=24 xmlns=http://www.w3.org/2000/svg><path d="M11.9997 10.8284L7.04996 15.7782L5.63574 14.364L11.9997 8L18.3637 14.364L16.9495 15.7782L11.9997 10.8284Z" fill=currentColor></path></svg></button></aside><main><div><div data-check-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24"><path d="M10.0007 15.1709L19.1931 5.97852L20.6073 7.39273L10.0007 17.9993L3.63672 11.6354L5.05093 10.2212L10.0007 15.1709Z" fill="currentColor"></path></svg>' data-copy-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24"><path d="M6.9998 6V3C6.9998 2.44772 7.44752 2 7.9998 2H19.9998C20.5521 2 20.9998 2.44772 20.9998 3V17C20.9998 17.5523 20.5521 18 19.9998 18H16.9998V20.9991C16.9998 21.5519 16.5499 22 15.993 22H4.00666C3.45059 22 3 21.5554 3 20.9991L3.0026 7.00087C3.0027 6.44811 3.45264 6 4.00942 6H6.9998ZM5.00242 8L5.00019 20H14.9998V8H5.00242ZM8.9998 6H16.9998V16H18.9998V4H8.9998V6Z" fill="currentColor"></path></svg>' id=copy-cfg style=display:none></div><article data-backlink-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="20" height="20"><path d="M9.41421 8L18.0208 16.6066L16.6066 18.0208L8 9.41421V17H6V6H17V8H9.41421Z" fill="currentColor"></path></svg>' class=prose><h1>Using LLMs to Transcribe and Translate (Part 1)</h1><div id=post-info><div id=date><span id=publish>2026-02-15</span></div><div id=tags><a href=https://mikansoro.org/tags/homelab><span>#</span>homelab</a><a href=https://mikansoro.org/tags/whisper><span>#</span>whisper</a><a href=https://mikansoro.org/tags/llm><span>#</span>llm</a></div></div><p>Languages are hard. They're complex systems of communication that take years to understand and many more to master.<p>Though I study Japanese when I can (read: I'm terrible at self-study motivation, so I don't study nearly as much as I should), my knowledge and understanding of the spoken and written word could at best be described as rudimentary; more often than not, it's closer to illiterate. Despite my slow progress, I still want to enjoy and understand various Japanese shows and radio programs at a deeper-than-surface level. Official translation outside of high-budget works such as Anime TV series or films are few and far between, and while fan translators are excellent and wonderful people who produce amazing, high quality work, they are still just people; individuals or small groups who translate the content they enjoy as a hobby or by commission. They do incredible work for those of us less linguistically inclined, but sometimes the content I'm looking to understand doesn't have a dedicated, overworked and thankless translator working into the wee hours to deliver localized translations of those works because they find it fun.<p>Before we go any further, allow me a moment to be candid: <em>This is not a post about how to replace fan translators or official translators with Large Language Models (LLMs). This is also not a post complaining that XYZ program or broadcast does not have a fan translator working on it yet.</em> The ethics surrounding the training of LLMs are poor, but they are a tool that when used properly can aid in both understanding and learning (in this post, both in a technical and linguistic sense).<p>If (hopefully when) I'm smart enough to actually understand (any of) the nuances of Japanese, I'd be filling in the gaps and publishing translations of things <em>I</em> found important too.<p>For the time being, I've been leveraging one of my favorite language models: <a rel="nofollow noreferrer" href=https://github.com/openai/whisper>whisper</a>. Released by OpenAI, whisper is a multilingual speech recognition model capable of both speech-to-text and text-to-speech conversion. We're interested in the former capability here, taking the spoken word from a radio show, youtube live stream, or TV show and converting it to text that we (or I) can understand. The model is small enough and fast enough to run on commodity desktop hardware. With the right machine, it's even capable of real-time transcription <em>and translation</em> of voice to text.<p>If that sounds like something either a) cool or b) useful to you, read on. I'll show you how I'm currently leveraging whisper (and other LLMs) to generate reasonable, understandable transcriptions and translations of the spoken word.<p>This will be a multi-part series. This is just an introduction post to the world of whisper and its tooling.<h1 id=environment>Environment<a aria-label="Anchor link for: environment" class=zola-anchor href=#environment style=visibility:hidden>#</a></h1><p>From this point on, we're going to get technical.<p>Whisper is a niche, specialized LLM focused entirely on transcribing speech-to-text. It's specialization is great for us commonfolk who don't own a data center; it's overall size is quite small (several gigabytes for the largest model). It comes in a variety of sizes (small-en, small, medium, large, etc) for use on single board computers like a raspberry pi to high end desktop and server hardware. The current front-runner version of the model, <code>large-v3</code>, takes up about 3GB on disk, and when expanded into system or video memory consumes about 7GB during workloads. Most tools built on whisper even support cpu-only inference at reasonable speeds.<p>This has a few advantages:<ul><li>any modern computer with a halfway decent CPU can transcribe video using the high-end (high parameter count) whisper models<li>a CUDA-compatible (NVIDIA) gpu with >=8GB of VRAM can load the entire model into video memory for high-speed operation</ul><p>The old laptop you have sitting in the corner can run Whisper, and that's a really cool thing.<h2 id=dependency-hell>Dependency Hell<a aria-label="Anchor link for: dependency-hell" class=zola-anchor href=#dependency-hell style=visibility:hidden>#</a></h2><p>One of the most challenging aspects of using whisper (and the tools developed around it) is managing the packages and dependencies required by each tool. To put it mildly, its dependency hell.<p>Many machine learning (ML) and LLM tools are built with Python due to its nearly endless suite of third party libraries and extensions. However, Python is an "interpreted" language instead of a "compiled" one, so instead of the developer sharing a built and packaged binary with you (like an <code>.exe</code> on Windows), your machine must have a version of Python <em>and</em> any dependencies the program author makes use of on your system every time you run the tool.<p>I've dealt with this in two ways: virtual environments and containers (docker). Which you choose is ultimately up to you and your comfort level, but <em>my</em> recommendation is to use containers. You'll save yourself a lot of headache in the long run.<h3 id=containers-podman-docker>Containers (Podman / Docker)<a aria-label="Anchor link for: containers-podman-docker" class=zola-anchor href=#containers-podman-docker style=visibility:hidden>#</a></h3><p>If you're unfamiliar with containers here's the elevator pitch on why they're useful:<blockquote><p>What is a container? Simply put, containers are isolated processes for each of your app's components. Each component - the frontend React app, the Python API engine, and the database - runs in its own isolated environment, completely isolated from everything else on your machine. <a rel="nofollow noreferrer" href=https://docs.docker.com/get-started/docker-concepts/the-basics/what-is-a-container/>read more</a></blockquote><p>They're small, predefined, prepackaged environments that are ready to run the tool installed in them. No managing dependent packages or 10 versions of Python. You run the container, and it runs the software exactly as the developer packaged it. Think of it like a mini operating system that you download from the internet, and run inside your main OS (Windows, macOS, or Linux). Multiple containers can run side by side, each with their own versions of software dependencies that will never conflict with each other.<p>They solve the "works on my machine" problem for those who don't want to get their hands (and operating systems) dirty.<p>To enter the magical world of containers, install either <code>podman</code> or <code>docker</code> onto your computer. I recommend <a rel="nofollow noreferrer" href=https://podman.io/>podman</a>, which for reasons outside the scope of this post is slowly gaining ground in the container desktop runtime space from docker (the "established" corporate player) due to its excellent feature set and backwards compatibility with the docker ecosystem.<p>For macOS and Windows, I recommend using <a rel="nofollow noreferrer" href=https://podman-desktop.io/>podman desktop</a> because it offers a good, UI-driven view of your podman install & running containers. It also offers step-by-step instructions for installing the podman runtime. As a note, on both Windows and Mac, podman (and docker) use a virtual machine to run container processes, and you'll have to configure that as part of the first-time setup. Once it's all configured, you don't have to touch it again.<p>If you need them, the <a rel="nofollow noreferrer" href=https://podman.io/docs/installation#installing-on-linux>linux install instructions</a> are here, along with CLI-only installation instructions for both Windows and Mac. Linux users can also make use of podman desktop, if that's your thing.<h3 id=python-virtual-environments>Python Virtual Environments<a aria-label="Anchor link for: python-virtual-environments" class=zola-anchor href=#python-virtual-environments style=visibility:hidden>#</a></h3><p>If containers aren't up your alley but you smartly don't want to install every required package under the sun to your global system state (otherwise known as <em>dependency hell</em>), python virtual environments will be your friend. I've had success with both <a rel="nofollow noreferrer" href=https://virtualenv.pypa.io/en/latest/>virtualenv</a> and <a rel="nofollow noreferrer" href=https://www.anaconda.com/download>anaconda</a> in the past; the former for managing project dependencies from <code>pip</code> (python's package manager) and the latter to manage versioned installations of python. Both have trade-offs (and this is not an exhaustive list), but for beginners I'd recommend starting with anaconda. Tools built around AI/LLMs use many different versions of python, so having a version manager for python itself is a good idea.<p>If you're a developer who regularly works with python, feel free to ignore my advice and advocate your position!<p>For windows <code>anaconda</code> users who find themselves needing access to their environments over ssh, execute the following after connecting to your windows machine:<pre class="language-cmd z-code" data-lang=cmd><code class=language-cmd data-lang=cmd><span class="z-source z-dosbatch">powershell.exe -ExecutionPolicy ByPass -NoExit -Command <span class="z-string z-quoted z-double z-dosbatch"><span class="z-punctuation z-definition z-string z-begin z-dosbatch">"</span>& 'C:\Users\&LTyouruser>\anaconda3\shell\condabin\conda-hook.ps1' ; conda activate 'C:\Users\&LTyouruser>\anaconda3' <span class="z-punctuation z-definition z-string z-end z-dosbatch">"</span></span>
</span></code></pre><p>This is the same command the "Anaconda Shell" start menu entry runs to launch anaconda.<p>Some resources to help get started with Anaconda:<ul><li><a rel="nofollow noreferrer" href=https://www.anaconda.com/docs/getting-started/main>Anaconda's Getting Started guide</a><li><code>conda</code> <a rel="nofollow noreferrer" href=https://docs.conda.io/projects/conda/en/stable/user-guide/cheatsheet.html>Command cheatsheet</a></ul><h2 id=gpu-acceleration>GPU Acceleration<a aria-label="Anchor link for: gpu-acceleration" class=zola-anchor href=#gpu-acceleration style=visibility:hidden>#</a></h2><p>Unless you live under a rock, you no doubt are aware that running AI inference workloads (like whisper) on a GPU speeds up the process dramatically. Whisper is a small model that runs fine on a CPU, but if you have a GPU with >=8GB of VRAM, you might as well try to accelerate it.<p>Each GPU vendor (Nvidia, AMD, Intel) have their own acceleration api (CUDA, rocm, and oneapi/openvino, respectively). Each API has its own setup steps for each operating system, so refer to the respective documentation for installation instructions. Make sure you have up-to-date GPU drivers for your card as well. There is also the Vulkan graphics api. which supports multiple GPU types which can be used as a fallback.<p>Frankly, the easiest path to GPU acceleration is to use containers. All of the libraries (like CUDA) are installed within the container instead of globally on the operating system, making multiple library versions a non-issue. I'll walk through my setup in a subsequent post, for both Nvidia CUDA and Vulkan.<h1 id=transcription>Transcription<a aria-label="Anchor link for: transcription" class=zola-anchor href=#transcription style=visibility:hidden>#</a></h1><p>Whisper itself is just a model so to actually <em>use</em> whisper we'll need some tools. Fortunately, other developers have built a plethora for us to choose from. I'll list a few of my favorites here:<ul><li><a rel="nofollow noreferrer" href=https://github.com/ggml-org/whisper.cpp>whisper.cpp</a> - a high-speed c++ command-line based whisper execution tool that works with practically every gpu/machine acceleration API currently in use<li><a rel="nofollow noreferrer" href=https://github.com/fortypercnt/stream-translator>stream-translator</a> - a tool for generating real-time subtitles from a live stream (like youtube or a <code>.m3u8</code> manifest)<li><a rel="nofollow noreferrer" href=https://gitlab.com/aadnk/whisper-webui/>whisper-webui</a> - a self-hostable web interface to running whisper that takes file uploads from the browser (or youtube URLs) to generate transcripts/translations. Very new-user friendly.</ul><p>There are many other tools to transcribe audio to text (and even translate it to english!) using whisper, but these are the ones I've used in the past.<h1 id=translation>Translation<a aria-label="Anchor link for: translation" class=zola-anchor href=#translation style=visibility:hidden>#</a></h1><p>While whisper can translate text to English in the same breath as transcribing in the audio's native language, the translations aren't high quality. They're serviceable, but you'll get better results shoving the transcribed text/subtitle file through another general purpose LLM (with internet access preferred) like Claude Sonnet, Gemini or ChatGPT. (Anecdotally, ChatGPT 5.x's translation quality is far below GPT4, so YMMV. I've had good luck with Claude via Kagi.)</article></div><footer><div class=copyright><p>Â© 2023 mikansoro</div><div class=credits>powered by <a rel="noreferrer noopener" href=https://www.getzola.org target=_blank>zola</a> and <a rel="noreferrer noopener" href=https://github.com/isunjn/serene target=_blank>serene</a></div></footer></main></div><script src=/js/lightense.min.js></script><script src=/js/main.js></script>